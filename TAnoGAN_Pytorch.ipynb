{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TAnoGAN: Time Series Anomaly Detection with Generative Adversarial Networks\n",
    "\n",
    "Paper link at arxib:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import torch.nn.init as init\n",
    "from torch.autograd import Variable\n",
    "import datetime\n",
    "from nab_dataset import NabDataset\n",
    "from models.recurrent_models_pyramid import LSTMGenerator, LSTMDiscriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Basic Settings for Adversarial Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArgsTrn:\n",
    "    workers=4\n",
    "    batch_size=32\n",
    "    epochs=20\n",
    "    lr=0.0002\n",
    "    cuda = True\n",
    "    manualSeed=2\n",
    "opt_trn=ArgsTrn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(opt_trn.manualSeed)\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location of datasets and category\n",
    "end_name = 'cpu_utilization_asg_misconfiguration.csv' # dataset name\n",
    "data_file = 'data\\\\realKnownCause\\\\'+end_name # dataset category and dataset name\n",
    "key = 'realKnownCause/'+end_name # This key is used for reading anomaly labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings for data loader\n",
    "class DataSettings:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.BASE = 'D:\\\\ResearchDataGtx1060\\\\AnomalyDetectionData\\\\NabDataset\\\\'\n",
    "        self.label_file = 'labels\\\\combined_windows.json'\n",
    "        self.data_file = data_file\n",
    "        self.key = key\n",
    "        self.train = True\n",
    "    \n",
    "    \n",
    "data_settings = DataSettings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset object and data loader object for NAB dataset\n",
    "dataset = NabDataset(data_settings=data_settings)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=opt_trn.batch_size,\n",
    "                                         shuffle=True, num_workers=int(opt_trn.workers))\n",
    "\n",
    "dataset.x.shape, dataset.y.shape # check the dataset shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if opt_trn.cuda else \"cpu\") # select the device\n",
    "seq_len = dataset.window_length # sequence length is equal to the window length\n",
    "in_dim = dataset.n_feature # input dimension is same as number of feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create generator and discriminator models\n",
    "netD = LSTMDiscriminator(in_dim=in_dim, device=device).to(device)\n",
    "netG = LSTMGenerator(in_dim=in_dim, out_dim=in_dim, device=device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"|Discriminator Architecture|\\n\", netD)\n",
    "print(\"|Generator Architecture|\\n\", netG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup loss function\n",
    "criterion = nn.BCELoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup optimizer\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=opt_trn.lr)\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=opt_trn.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adversarial Training of Generator and Discriminator Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "for epoch in range(opt_trn.epochs):\n",
    "    for i, (x,y) in enumerate(dataloader, 0):\n",
    "        \n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "\n",
    "        #Train with real data\n",
    "        netD.zero_grad()\n",
    "        real = x.to(device)\n",
    "        batch_size, seq_len = real.size(0), real.size(1)\n",
    "        label = torch.full((batch_size, seq_len, 1), real_label, device=device)\n",
    "\n",
    "        output,_ = netD.forward(real)\n",
    "        errD_real = criterion(output, label)\n",
    "        errD_real.backward()\n",
    "        optimizerD.step()\n",
    "        D_x = output.mean().item()\n",
    "        \n",
    "        #Train with fake data\n",
    "        noise = Variable(init.normal(torch.Tensor(batch_size,seq_len,in_dim),mean=0,std=0.1)).cuda()\n",
    "        fake,_ = netG.forward(noise)\n",
    "        output,_ = netD.forward(fake.detach()) # detach causes gradient is no longer being computed or stored to save memeory\n",
    "        label.fill_(fake_label)\n",
    "        errD_fake = criterion(output, label)\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        errD = errD_real + errD_fake\n",
    "        optimizerD.step()\n",
    "        \n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        noise = Variable(init.normal(torch.Tensor(batch_size,seq_len,in_dim),mean=0,std=0.1)).cuda()\n",
    "        fake,_ = netG.forward(noise)\n",
    "        label.fill_(real_label) \n",
    "        output,_ = netD.forward(fake)\n",
    "        errG = criterion(output, label)\n",
    "        errG.backward()\n",
    "        optimizerG.step()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        \n",
    "        \n",
    "\n",
    "    print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f' \n",
    "          % (epoch, opt_trn.epochs, i, len(dataloader),\n",
    "             errD.item(), errG.item(), D_x, D_G_z1, D_G_z2), end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define basic settings for inverse mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArgsTest:\n",
    "    workers = 1\n",
    "    batch_size = 1\n",
    "    \n",
    "opt_test=ArgsTest()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = netG # changing reference variable \n",
    "discriminator = netD # changing reference variable "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define settings for loading data in evaluation mood\n",
    "class TestDataSettings:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.BASE = 'D:\\\\ResearchDataGtx1060\\\\AnomalyDetectionData\\\\NabDataset\\\\'\n",
    "        self.label_file = 'labels\\\\combined_windows.json'\n",
    "        self.data_file = data_file\n",
    "        self.key = key\n",
    "        self.train = False\n",
    "\n",
    "        \n",
    "test_data_settings = TestDataSettings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset object and data loader object in evaluation mood for NAB dataset\n",
    "\n",
    "test_dataset = NabDataset(test_data_settings)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=opt_test.batch_size, \n",
    "                                         shuffle=False, num_workers=int(opt_test.workers))\n",
    "\n",
    "test_dataset.x.shape, test_dataset.y.shape, test_dataset.data_len # check the dataset shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a function to calculate anomaly score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lambda = 0.1 according to paper\n",
    "# x is new data, G_z is closely regenerated data\n",
    "\n",
    "def Anomaly_score(x, G_z, Lambda=0.1):\n",
    "    residual_loss = torch.sum(torch.abs(x-G_z)) # Residual Loss\n",
    "    \n",
    "    # x_feature is a rich intermediate feature representation for real data x\n",
    "    output, x_feature = discriminator(x.to(device)) \n",
    "    # G_z_feature is a rich intermediate feature representation for fake data G(z)\n",
    "    output, G_z_feature = discriminator(G_z.to(device)) \n",
    "    \n",
    "    discrimination_loss = torch.sum(torch.abs(x_feature-G_z_feature)) # Discrimination loss\n",
    "    \n",
    "    total_loss = (1-Lambda)*residual_loss.to(device) + Lambda*discrimination_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse mapping to latent space and reconstruction of data for estimating anomaly score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list = []\n",
    "#y_list = []\n",
    "for i, (x,y) in enumerate(test_dataloader):\n",
    "    print(i, y)\n",
    "    \n",
    "    z = Variable(init.normal(torch.zeros(opt_test.batch_size,\n",
    "                                     test_dataset.window_length, \n",
    "                                     test_dataset.n_feature),mean=0,std=0.1),requires_grad=True)\n",
    "    #z = x\n",
    "    z_optimizer = torch.optim.Adam([z],lr=1e-2)\n",
    "    \n",
    "    loss = None\n",
    "    for j in range(50): # set your interation range\n",
    "        gen_fake,_ = generator(z.cuda())\n",
    "        loss = Anomaly_score(Variable(x).cuda(), gen_fake)\n",
    "        loss.backward()\n",
    "        z_optimizer.step()\n",
    "\n",
    "    loss_list.append(loss) # Store the loss from the final iteration\n",
    "    #y_list.append(y) # Store the corresponding anomaly label\n",
    "    print('~~~~~~~~loss={},  y={} ~~~~~~~~~~'.format(loss, y))\n",
    "    #break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "THRESHOLD = 7.65 # Anomaly score threshold for an instance to be considered as anomaly \n",
    "\n",
    "#TIME_STEPS = dataset.window_length\n",
    "test_score_df = pd.DataFrame(index=range(test_dataset.data_len))\n",
    "test_score_df['loss'] = [loss.item()/test_dataset.window_length for loss in loss_list]\n",
    "test_score_df['y'] = test_dataset.y\n",
    "test_score_df['threshold'] = THRESHOLD\n",
    "test_score_df['anomaly'] = test_score_df.loss > test_score_df.threshold\n",
    "test_score_df['t'] = [x[59].item() for x in test_dataset.x]\n",
    "\n",
    "plt.plot(test_score_df.index, test_score_df.loss, label='loss')\n",
    "plt.plot(test_score_df.index, test_score_df.threshold, label='threshold')\n",
    "plt.plot(test_score_df.index, test_score_df.y, label='y')\n",
    "plt.xticks(rotation=25)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "anomalies = test_score_df[test_score_df.anomaly == True]\n",
    "\n",
    "plt.plot(\n",
    "  range(test_dataset.data_len), \n",
    "  test_score_df['t'], \n",
    "  label='value'\n",
    ");\n",
    "\n",
    "sns.scatterplot(\n",
    "  anomalies.index,\n",
    "  anomalies.t,\n",
    "  color=sns.color_palette()[3],\n",
    "  s=52,\n",
    "  label='anomaly'\n",
    ")\n",
    "\n",
    "plt.plot(\n",
    "  range(len(test_score_df['y'])),\n",
    "  test_score_df['y'],\n",
    "  label='y'\n",
    ")\n",
    "\n",
    "plt.xticks(rotation=25)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the window-based anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "start_end = []\n",
    "state = 0\n",
    "for idx in test_score_df.index:\n",
    "    if state==0 and test_score_df.loc[idx, 'y']==1:\n",
    "        state=1\n",
    "        start = idx\n",
    "    if state==1 and test_score_df.loc[idx, 'y']==0:\n",
    "        state = 0\n",
    "        end = idx\n",
    "        start_end.append((start, end))\n",
    "\n",
    "for s_e in start_end:\n",
    "    if sum(test_score_df[s_e[0]:s_e[1]+1]['anomaly'])>0:\n",
    "        for i in range(s_e[0], s_e[1]+1):\n",
    "            test_score_df.loc[i, 'anomaly'] = 1\n",
    "            \n",
    "actual = np.array(test_score_df['y'])\n",
    "predicted = np.array([int(a) for a in test_score_df['anomaly']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate measurement scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "\n",
    "predicted = np.array(predicted)\n",
    "actual = np.array(actual)\n",
    "\n",
    "tp = np.count_nonzero(predicted * actual)\n",
    "tn = np.count_nonzero((predicted - 1) * (actual - 1))\n",
    "fp = np.count_nonzero(predicted * (actual - 1))\n",
    "fn = np.count_nonzero((predicted - 1) * actual)\n",
    "\n",
    "print('True Positive\\t', tp)\n",
    "print('True Negative\\t', tn)\n",
    "print('False Positive\\t', fp)\n",
    "print('False Negative\\t', fn)\n",
    "\n",
    "accuracy = (tp + tn) / (tp + fp + fn + tn)\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "fmeasure = (2 * precision * recall) / (precision + recall)\n",
    "cohen_kappa_score = cohen_kappa_score(predicted, actual)\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(actual, predicted)\n",
    "auc_val = auc(false_positive_rate, true_positive_rate)\n",
    "roc_auc_val = roc_auc_score(actual, predicted)\n",
    "\n",
    "print('Accuracy\\t', accuracy)\n",
    "print('Precision\\t', precision)\n",
    "print('Recall\\t', recall)\n",
    "print('f-measure\\t', fmeasure)\n",
    "print('cohen_kappa_score\\t', cohen_kappa_score)\n",
    "print('auc\\t', auc_val)\n",
    "print('roc_auc\\t', roc_auc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import winsound\n",
    "# frequency = 200  # Set Frequency To 2500 Hertz\n",
    "# duration = 2000  # Set Duration To 1000 ms == 1 second\n",
    "# winsound.Beep(frequency, duration)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
