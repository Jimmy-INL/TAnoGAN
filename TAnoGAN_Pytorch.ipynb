{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAnoGAN\n",
    "\n",
    "### Problem: Anomaly  Detection in Time Series Data\n",
    "\n",
    "#### Technique: (a) Generative Adversarial Network (GAN) and (b) Anomaly Score by Mapping Real Data to Latent Space\n",
    "\n",
    "Language: Python\n",
    "\n",
    "Library: PyTorch\n",
    "\n",
    "This is the code for TAnoGAN model by Bashar and Nayak et al. used for anomaly detection in time series data\n",
    "\n",
    "Data at Kaggle: https://www.kaggle.com/boltzmannbrain/nab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import torch.nn.init as init\n",
    "from torch.autograd import Variable\n",
    "import datetime\n",
    "from nab_dataset import NabDataset\n",
    "from models.recurrent_models import LSTMGenerator, LSTMDiscriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Basic Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArgsTrn:\n",
    "    workers=4\n",
    "    batch_size=32\n",
    "    epochs=10\n",
    "    lr=0.0002\n",
    "    cuda = True\n",
    "    outf='checkpoints'\n",
    "    imf='images'\n",
    "    manualSeed=2\n",
    "opt_trn=ArgsTrn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(opt_trn.manualSeed)\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the Training Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_name = 'exchange-4_cpm_results.csv'\n",
    "data_file = 'data\\\\realAdExchange\\\\'+end_name\n",
    "key = 'realAdExchange/'+end_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings for data loader\n",
    "class DataSettings:\n",
    "    def __init__(self):\n",
    "        self.BASE = 'NabDataset\\\\'\n",
    "        self.label_file = 'labels\\\\combined_windows.json'\n",
    "        self.data_file = data_file\n",
    "        self.key = key\n",
    "        self.train = True\n",
    "data_settings = DataSettings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = NabDataset(data_settings=data_settings)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=opt_trn.batch_size,\n",
    "                                         shuffle=False, num_workers=int(opt_trn.workers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.x.shape, dataset.y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if opt_trn.cuda else \"cpu\")\n",
    "seq_len = dataset.window_length\n",
    "in_dim = nz = dataset.n_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netD = LSTMDiscriminator(in_dim=in_dim, hidden_dim=100, device=device).to(device)\n",
    "netG = LSTMGenerator(in_dim=in_dim, out_dim=in_dim, hidden_dim=100, device=device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"|Discriminator Architecture|\\n\", netD)\n",
    "print(\"|Generator Architecture|\\n\", netG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup loss function\n",
    "criterion = nn.BCELoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup optimizer\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=opt_trn.lr)\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=opt_trn.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "for epoch in range(opt_trn.epochs):\n",
    "    for i, (x,y) in enumerate(dataloader, 0):\n",
    "        \n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "\n",
    "        #Train with real data\n",
    "        netD.zero_grad()\n",
    "        real = x.to(device)\n",
    "        batch_size, seq_len = real.size(0), real.size(1)\n",
    "        label = torch.full((batch_size, seq_len, 1), real_label, device=device)\n",
    "\n",
    "        output = netD(real)\n",
    "        errD_real = criterion(output, label)\n",
    "        errD_real.backward()\n",
    "        optimizerD.step()\n",
    "        D_x = output.mean().item()\n",
    "        \n",
    "        #Train with fake data\n",
    "        noise = Variable(init.normal(torch.Tensor(batch_size,seq_len,in_dim),mean=0,std=0.1)).cuda()\n",
    "        fake = netG.forward(noise)\n",
    "        output = netD.forward(fake.detach()) # detach causes gradient is no longer being computed or stored to save memeory\n",
    "        label.fill_(fake_label)\n",
    "        errD_fake = criterion(output, label)\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        errD = errD_real + errD_fake\n",
    "        optimizerD.step()\n",
    "        \n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        noise = Variable(init.normal(torch.Tensor(batch_size,seq_len,in_dim),mean=0,std=0.1)).cuda()\n",
    "        fake = netG.forward(noise)\n",
    "        label.fill_(real_label) \n",
    "        output = netD.forward(fake)\n",
    "        errG = criterion(output, label)\n",
    "        errG.backward()\n",
    "        optimizerG.step()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        \n",
    "    print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f' \n",
    "          % (epoch, opt_trn.epochs, i, len(dataloader),\n",
    "             errD.item(), errG.item(), D_x, D_G_z1, D_G_z2), end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from nab_dataset import NabDataset\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.nn.init as init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In this step we can restore model for testing. For simplicity we are omiting that step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = netG\n",
    "discriminator = netD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings for data loader\n",
    "class TestDataSettings:\n",
    "    def __init__(self):\n",
    "        self.BASE = 'NabDataset\\\\'\n",
    "        self.label_file = 'labels\\\\combined_windows.json'\n",
    "        self.data_file = data_file\n",
    "        self.key = key\n",
    "        self.train = False\n",
    "\n",
    "test_data_settings = TestDataSettings()\n",
    "test_dataset = NabDataset(test_data_settings)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=opt_test.batch_size, \n",
    "                                         shuffle=False, num_workers=int(opt_test.workers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.x.shape, test_dataset.y.shape, test_dataset.data_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define a function to calculate anomaly score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lambda = 0.1\n",
    "# x is new data, G_z is closely regenerated data\n",
    "\n",
    "def Anomaly_score(x, G_z, Lambda=0.1):\n",
    "    residual_loss = torch.sum(torch.abs(x-G_z))\n",
    "    \n",
    "    x_feature = discriminator(x.to(device))#This can be updated as the paper\n",
    "    G_z_feature = discriminator(G_z.to(device))#This can be updated as the paper\n",
    "    \n",
    "    discrimination_loss = torch.sum(torch.abs(x_feature-G_z_feature))\n",
    "    \n",
    "    total_loss = (1-Lambda)*residual_loss.to(device) + Lambda*discrimination_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test with the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list = []\n",
    "y_list = []\n",
    "for i, (x,y) in enumerate(test_dataloader):\n",
    "    print(i, y)\n",
    "    \n",
    "    z = Variable(init.normal(torch.zeros(opt_test.batch_size,\n",
    "                                     test_dataset.window_length, \n",
    "                                     test_dataset.n_feature),mean=0,std=0.1),requires_grad=True)\n",
    "    #z = x\n",
    "    z_optimizer = torch.optim.Adam([z],lr=1e-2)\n",
    "    \n",
    "    loss = None\n",
    "    for j in range(50): # set your interation range\n",
    "        gen_fake = generator(z.cuda())\n",
    "        loss = Anomaly_score(Variable(x).cuda(), gen_fake)\n",
    "        loss.backward()\n",
    "        z_optimizer.step()\n",
    "\n",
    "    loss_list.append(loss)\n",
    "    y_list.append(y)\n",
    "    print('~~~~~~~~loss={},  y={} ~~~~~~~~~~'.format(loss, y))\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "THRESHOLD = 1\n",
    "\n",
    "TIME_STEPS = dataset.window_length\n",
    "test_score_df = pd.DataFrame(index=range(test_dataset.data_len))\n",
    "test_score_df['loss'] = [loss.item()/test_dataset.window_length for loss in loss_list]\n",
    "test_score_df['y'] = test_dataset.y\n",
    "test_score_df['threshold'] = THRESHOLD\n",
    "test_score_df['anomaly'] = test_score_df.loss > test_score_df.threshold\n",
    "test_score_df['t'] = [x[59].item() for x in test_dataset.x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(test_score_df.index, test_score_df.loss, label='loss')\n",
    "plt.plot(test_score_df.index, test_score_df.threshold, label='threshold')\n",
    "plt.plot(test_score_df.index, test_score_df.y, label='y')\n",
    "plt.xticks(rotation=25)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies = test_score_df[test_score_df.anomaly == True]\n",
    "anomalies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.plot(\n",
    "  range(test_dataset.data_len), \n",
    "  test_score_df['t'], \n",
    "  label='value'\n",
    ");\n",
    "\n",
    "sns.scatterplot(\n",
    "  anomalies.index,\n",
    "  anomalies.t,\n",
    "  color=sns.color_palette()[3],\n",
    "  s=52,\n",
    "  label='anomaly'\n",
    ")\n",
    "\n",
    "plt.plot(\n",
    "  range(len(test_score_df['y'])),\n",
    "  test_score_df['y'],\n",
    "  label='y'\n",
    ")\n",
    "\n",
    "plt.xticks(rotation=25)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "start_end = []\n",
    "state = 0\n",
    "for idx in test_score_df.index:\n",
    "    if state==0 and test_score_df.loc[idx, 'y']==1:\n",
    "        state=1\n",
    "        start = idx\n",
    "    if state==1 and test_score_df.loc[idx, 'y']==0:\n",
    "        state = 0\n",
    "        end = idx\n",
    "        start_end.append((start, end))\n",
    "\n",
    "for s_e in start_end:\n",
    "    if sum(test_score_df[s_e[0]:s_e[1]+1]['anomaly'])>0:\n",
    "        for i in range(s_e[0], s_e[1]+1):\n",
    "            test_score_df.loc[i, 'anomaly'] = 1\n",
    "            \n",
    "actual = np.array(test_score_df['y'])\n",
    "predicted = np.array([int(a) for a in test_score_df['anomaly']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "\n",
    "predicted = np.array(predicted)\n",
    "actual = np.array(actual)\n",
    "\n",
    "tp = np.count_nonzero(predicted * actual)\n",
    "tn = np.count_nonzero((predicted - 1) * (actual - 1))\n",
    "fp = np.count_nonzero(predicted * (actual - 1))\n",
    "fn = np.count_nonzero((predicted - 1) * actual)\n",
    "\n",
    "print('True Positive\\t', tp)\n",
    "print('True Negative\\t', tn)\n",
    "print('False Positive\\t', fp)\n",
    "print('False Negative\\t', fn)\n",
    "\n",
    "accuracy = (tp + tn) / (tp + fp + fn + tn)\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "fmeasure = (2 * precision * recall) / (precision + recall)\n",
    "cohen_kappa_score = cohen_kappa_score(predicted, actual)\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(actual, predicted)\n",
    "auc_val = auc(false_positive_rate, true_positive_rate)\n",
    "roc_auc_val = roc_auc_score(actual, predicted)\n",
    "\n",
    "print('Accuracy\\t', accuracy)\n",
    "print('Precision\\t', precision)\n",
    "print('Recall\\t', recall)\n",
    "print('f-measure\\t', fmeasure)\n",
    "print('cohen_kappa_score\\t', cohen_kappa_score)\n",
    "print('auc\\t', auc_val)\n",
    "print('roc_auc\\t', roc_auc_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
